{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules and libraries in the code. \n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os \n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the huggin face model. \n",
    "dataset = load_dataset(\"jacob-hugging-face/job-descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the output and the data format of the dataset. The data was in the form of Nested Dictionary.\n",
    "print(dataset['train'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Dataset\n",
    "\n",
    "The following code fetches the imported features of the dataset and stores the same in json format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionry data into json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to properly transform the data to later save it into json format. \n",
    "def transform_to_horizontal(data, fields, limit=None):\n",
    "\n",
    "    # Initialzie list to hold the dataq. \n",
    "    horizontal_data = []\n",
    "    \n",
    "    # Use teh enumerate method to get the incdex as well as the item. \n",
    "    for idx, item in enumerate(data):\n",
    "\n",
    "        # Break the loop if it crosses the limit. \n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "        # Append the items into the dictionary. \n",
    "        horizontal_item = {field: item[field] for field in fields}\n",
    "        # Append the dictionary data as elements of the list. \n",
    "        horizontal_data.append(horizontal_item)\n",
    "\n",
    "    # Return the list object. \n",
    "    return horizontal_data\n",
    "\n",
    "# Specify the fields you want to include in the horizontal format\n",
    "fields_to_include = [\"company_name\", \"job_description\", \"position_title\", \"description_length\", \"model_response\"]\n",
    "\n",
    "# Transform the data to horizontal format\n",
    "horizontal_data = transform_to_horizontal(dataset[\"train\"], fields_to_include, limit=15)\n",
    "\n",
    "# Saving the horizontal data to a JSON file\n",
    "with open(\"job_descriptions/cv_data.json\", \"w\") as file:\n",
    "    # Dump the json data into a file. \n",
    "    json.dump(horizontal_data, file)\n",
    "    # Its always a good practice to close the file after the operations are done. \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Details From The CVs\n",
    "\n",
    "The following codes helps to extract meaningful information from the PDFs. This may not be 100% accurate but contains reasonable and meaningful information from the CVS. After the extraction process is complete the data is stored in json format in the extracted_details.json file of the extracted folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data extraction from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path from which you need to extract all the PDFs. \n",
    "PATH=\"archive/data/data\"\n",
    "\n",
    "# Extract all the directories under the mentioned path using list comprehension method. \n",
    "items = os.listdir(PATH)\n",
    "directories = [item for item in items if os.path.isdir(os.path.join(PATH, item))]\n",
    "\n",
    "print(\"Directories in the path:\")\n",
    "\n",
    "# Initialize a list which will hold the list of all the categories (basically the names of the directories)\n",
    "list_of_categories=[]\n",
    "\n",
    "for directory in directories:\n",
    "    list_of_categories.append(directory)\n",
    "    # print(directory)\n",
    "print(list_of_categories)\n",
    "\n",
    "\n",
    "# Function to extract category, skills, and education from a PDF\n",
    "def extract_details(pdf_path, category):\n",
    "    print(pdf_path)\n",
    "    # Wrap the code in try catch block to avoid any error. \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Initialize variables to store extracted details\n",
    "            experience = None\n",
    "            skills = []\n",
    "            education = []\n",
    "\n",
    "            # Iterate through pages in the PDF\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # Search for patterns in the extracted text\n",
    "                if \"Experience\" in text:\n",
    "                    experience = text.split(\"Experience\")[1].strip()\n",
    "                if \"Skills\" in text:\n",
    "                    skills = [skill.strip() for skill in text.split(\"Skills\")[1].split(\",\")]\n",
    "                if \"Education\" in text:\n",
    "                    education = [edu.strip() for edu in text.split(\"Education\")[1].split(\";\")]\n",
    "            # Return the data in the form of dictionary. \n",
    "            return {\n",
    "                'PDFFilename': os.path.basename(pdf_path), # Include the PDF filename\n",
    "                'Category': category,\n",
    "                'Experience': experience,\n",
    "                'Skills': skills,\n",
    "                'Education': education,\n",
    "            }\n",
    "    # Create an exception and return None in that case. \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details from {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create a list to store extracted details\n",
    "all_details = []\n",
    "\n",
    "for directory in list_of_categories:\n",
    "    # Directory containing PDF CVs\n",
    "    pdf_directory = f'archive/data/data/{directory}'\n",
    "    # Iterate through PDF files and extract details\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            details = extract_details(pdf_path, directory)\n",
    "            if details:\n",
    "                print(f\"Details extracted from {filename}:\\n{details}\\n\")\n",
    "                all_details.append(details)\n",
    "\n",
    "# Save the extracted details in a JSON file\n",
    "output_file = 'extracted/extracted_details.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    # Write the json data into a file. \n",
    "    json.dump(all_details, json_file, indent=4)\n",
    "\n",
    "    # Close the file.\n",
    "    json_file.close()\n",
    "# Print the output file \n",
    "print(f\"Extracted details saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find The Domain For Which Company Is Looking For\n",
    "\n",
    "A slight varition is done while implementing the solution. Instead of iterating all the CVs for finding the similarity it is better to first determine the category or the domain for which the company is lookin for. For ecample if the company is looking for Web developer then there is no point is processing the CVs that belong to teaching. The idea is to first determine the domain which best suits the role and later find the similarity matrix from only the relevant CVs. This helps to speed up the process by ~5 times as well as improve the accuracy of the algorithm being used. \n",
    "\n",
    "The data is stored in json format in the matched.json file. The file contains the company name along with the role for which they are looking for. \n",
    "\n",
    "The idea is to find the similarity matrix between the 'job_title' field and the 'Category'. Next the pairs having the heighest similarity score is considered and **only those CVs are selected which belongs to the category later.**\n",
    "\n",
    "**Note**: This is not 100% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the code to produce the json file containing the comapany and the category of the role they are looking for. \n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load your CV data from cv_data.json\n",
    "data = {}\n",
    "with open(\"job_descriptions/cv_data.json\", \"r\") as file:\n",
    "    # Load json data from the file. \n",
    "    data = json.load(file)\n",
    "    # Close the file. \n",
    "    file.close()\n",
    "\n",
    "# Extract company names and job descriptions. This will hold the company name along with the job description.\n",
    "company_and_job_descriptions = {}\n",
    "for item in data:\n",
    "    company_and_job_descriptions[item['company_name']] = item['position_title']\n",
    "\n",
    "# Load DistilBERT tokenizer and model on the GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Use the distilbert-base-uncased model \n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# Load job descriptions and extracted CV details from JSON\n",
    "with open('extracted/extracted_details.json', 'r') as json_file:\n",
    "    # Load the json file. \n",
    "    cv_details = json.load(json_file)\n",
    "    # Close the file. \n",
    "    json_file.close()\n",
    "categories = []\n",
    "for cv in cv_details:\n",
    "    categories.append(cv['Category'])\n",
    "\n",
    "# Convert the list of categories to a set to get unique categories\n",
    "unique_categories = list(set(categories))\n",
    "\n",
    "# Print the unique categories\n",
    "print(unique_categories)\n",
    "\n",
    "# Create a list of job descriptions\n",
    "job_descriptions = list(company_and_job_descriptions.values())\n",
    "\n",
    "\n",
    "# Tokenize and embed job descriptions using the tokenizer, model and the list comprehension technique. \n",
    "job_desc_embeddings = [model(**tokenizer(job_desc, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1) for job_desc in job_descriptions]\n",
    "\n",
    "# Initialize a dictionary to store collected CVs for each job description\n",
    "collected_cvs = {job_desc: [] for job_desc in job_descriptions}\n",
    "\n",
    "\n",
    "categories = []\n",
    "for cv in cv_details:\n",
    "    categories.append(cv['Category'])\n",
    "\n",
    "# Convert the list of categories to a set to get unique categories\n",
    "unique_categories = list(set(categories))\n",
    "\n",
    "# Print the unique categories\n",
    "\n",
    "\n",
    "store={}\n",
    "\n",
    "# Create a dictionary to store the similarity scores for each CV\n",
    "similarities = {}\n",
    "\n",
    "# Iterate over company names and job descriptions\n",
    "for company_name, job_desc in company_and_job_descriptions.items():\n",
    "\n",
    "    \n",
    "    # Calculate embeddings for the job description\n",
    "    job_desc_embedding = job_desc_embeddings[job_descriptions.index(job_desc)].squeeze(0)\n",
    "\n",
    "    # Iterate over CVs\n",
    "    for cv in unique_categories:\n",
    "        cv_text = f\"{cv}\"\n",
    "\n",
    "        # Create embeddings for the CV\n",
    "        cv_embedding = model(**tokenizer(cv_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1)\n",
    "        cv_embedding = cv_embedding.squeeze(0)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = torch.nn.functional.cosine_similarity(job_desc_embedding, cv_embedding, dim=0).item()\n",
    "\n",
    "        # Store the similarity score in the dictionary\n",
    "        if company_name not in similarities:\n",
    "            similarities[company_name] = {}\n",
    "        similarities[company_name][cv] = similarity\n",
    "\n",
    "# Find the top CV for each job description\n",
    "top_matching_cv = {}\n",
    "for company_name, similarity_scores in similarities.items():\n",
    "    top_cv = max(similarity_scores, key=similarity_scores.get)\n",
    "    top_matching_cv[company_name] = (top_cv, similarity_scores[top_cv])\n",
    "\n",
    "# Print the top matching CV for each job description\n",
    "for company_name, (top_cv, similarity) in top_matching_cv.items():\n",
    "    # print(f\"Company: {company_name}\")\n",
    "    # print(f\"Top Matching CV: {top_cv}\")\n",
    "    # print(f\"Similarity Score: {similarity}\")\n",
    "    store[company_name]=top_cv\n",
    "\n",
    "\n",
    "print(\"Companies and the domain they are looking for: {}\".format(store))\n",
    "\n",
    "with open(\"matches/matched.json\", \"w\") as file:\n",
    "    json.dump(store, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocessing along with the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory for Memory optimization. \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find The Similarity Matrix Between The CVs And Company Description\n",
    "\n",
    "In the following code we have used the PyTorch to find the cosine similarity matrix between the CVs and company description. Only the relevant CVs are selected for the process. The CVs are sorted as per their similarity matrix. Then the CVs with the heighest similarity is considered. 5 top CVs are selected for each company but the parameter can be changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load your CV data from cv_data.json\n",
    "data = {}\n",
    "with open(\"job_descriptions/cv_data.json\", \"r\") as file:\n",
    "    # Load json data from the file. \n",
    "    data = json.load(file)\n",
    "    # Close the file. \n",
    "    file.close()\n",
    "\n",
    "# Extract company names and job descriptions. This will hold the company name along with the job description.\n",
    "company_and_job_descriptions = {}\n",
    "for item in data:\n",
    "    company_and_job_descriptions[item['company_name']] = item['model_response']\n",
    "\n",
    "# Load DistilBERT tokenizer and model on the GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Use the distilbert-base-uncased model \n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# Load job descriptions and extracted CV details from JSON\n",
    "with open('extracted/extracted_details.json', 'r') as json_file:\n",
    "    # Load the json file. \n",
    "    cv_details = json.load(json_file)\n",
    "    # Close the file. \n",
    "    json_file.close()\n",
    "\n",
    "# Create a list of job descriptions\n",
    "job_descriptions = list(company_and_job_descriptions.values())\n",
    "\n",
    "\n",
    "# Tokenize and embed job descriptions using the tokenizer, model and the list comprehension technique. \n",
    "job_desc_embeddings = [model(**tokenizer(job_desc, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1) for job_desc in job_descriptions]\n",
    "\n",
    "# Initialize a dictionary to store collected CVs for each job description\n",
    "collected_cvs = {job_desc: [] for job_desc in job_descriptions}\n",
    "\n",
    "# Tokenize and embed CV details\n",
    "for company_name, job_desc in company_and_job_descriptions.items():\n",
    "    def return_selected_cvs(category):\n",
    "        selected_cvs = []  # Create an empty list to store selected CV details\n",
    "\n",
    "        # Load job descriptions and extracted CV details from JSON\n",
    "        with open('extracted/extracted_details.json', 'r') as json_file:\n",
    "            cv_details = json.load(json_file)\n",
    "\n",
    "            # Iterate over each CV detail\n",
    "            for cv_detail in cv_details:\n",
    "                 # Check if the \"Category\" key has the value of the passed category\n",
    "                if cv_detail.get(\"Category\") == category:\n",
    "                    selected_cvs.append(cv_detail)  # Add the CV detail to the selected_cvs list\n",
    "\n",
    "        return selected_cvs\n",
    "    \n",
    "    # Program to extract the directory to choose \n",
    "\n",
    "    comapany_preferred_domain={}\n",
    "    with open(\"matches/matched.json\", \"r\") as file:\n",
    "        comapany_preferred_domain=json.load(file)\n",
    "        file.close()\n",
    "    if(company_name in comapany_preferred_domain):\n",
    "        print(f\"Company name: {company_name}, {comapany_preferred_domain[company_name]}\")\n",
    "        domain=comapany_preferred_domain[company_name]\n",
    "    \n",
    "    cv_details = return_selected_cvs(domain)\n",
    "\n",
    "\n",
    "    \n",
    "    # Iterate over job descriptions\n",
    "    for cv in cv_details:\n",
    "\n",
    "        '''Use the join method to add categoiry, skills, and education as text data. This will be the raw text to be tokenized representing most of the information of the user. '''\n",
    "\n",
    "        cv_text = f\"{cv['Category']} {cv['Experience']} {', '.join(cv['Skills'])} {', '.join(cv['Education'])}\"\n",
    "\n",
    "        # Create embeddings of the raw text processed in the earlier step. The model is made to run on GPU if available. \n",
    "        # Padding is set to true to match the text datas. truncation is set to True to ensuer that the texts lies within a max limit. Max length defines the maximum length of the texts. \n",
    "        cv_embedding = model(**tokenizer(cv_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Calculate cosine similarity between job descriptions and CVs using PyTorch\n",
    "        cv_embedding = cv_embedding.squeeze(0)  # Remove the batch dimension\n",
    "\n",
    "        job_desc_embedding = job_desc_embeddings[job_descriptions.index(job_desc)].squeeze(0)  # Get the corresponding job description embedding\n",
    "        \n",
    "        # Perform similarity operation using the cosine similary of PyTorch. Note that I used this instead of the sklearn implementation to run the operation of GPU devices. This will help to perform the operation much faster. \n",
    "        similarity = torch.nn.functional.cosine_similarity(job_desc_embedding, cv_embedding, dim=0).item()\n",
    "\n",
    "        # Store the CV and similarity score\n",
    "        collected_cvs[job_desc].append((cv['PDFFilename'], similarity))\n",
    "\n",
    "def recommend_top_k_cvs(collected_cvs,k):\n",
    "    # Initialize a dictionary to store top 5 CVs for each job description\n",
    "    top_k_cvs = {}\n",
    "    # Sort the collected CVs by similarity score and select the top 5\n",
    "    for job_desc, cvs in collected_cvs.items():\n",
    "        top_k_cvs[job_desc] = sorted(cvs, key=lambda x: x[1], reverse=True)[:k]\n",
    "   \n",
    "    return top_k_cvs\n",
    "\n",
    "top_k_cvs=recommend_top_k_cvs(collected_cvs,5)\n",
    "\n",
    "# Function to find the key (company name) for a given job description\n",
    "def find_the_key(job_description):\n",
    "    for key, value in company_and_job_descriptions.items():\n",
    "        if value == job_description:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 CVs for each job description\n",
    "\n",
    "shortlisted_cvs={}\n",
    "for job_desc, cvs in top_k_cvs.items():\n",
    "\n",
    "    # Call the function to find the company name corresponding to the job description. \n",
    "    company = find_the_key(job_desc)\n",
    "    print(f\"Top 5 CVs for '{company}':\")\n",
    "\n",
    "    # List to store all the selected resumes. \n",
    "    list_of_selected_resumes=[]\n",
    "    for cv, similarity in cvs:\n",
    "        print(f\"CV: {cv}, Similarity Score: {similarity}\")\n",
    "        list_of_selected_resumes.append(cv)\n",
    "    shortlisted_cvs[company]=list_of_selected_resumes\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortlisted_cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cvs of the final shortlisted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shortlisted/shortlisted_cvs.json\", \"w\") as file:\n",
    "    json.dump(shortlisted_cvs, file, indent=4)\n",
    "    # Close the file. \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
