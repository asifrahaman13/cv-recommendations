{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jacob-hugging-face/job-descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionry data into json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data into horizontal format\n",
    "import json\n",
    "horizontal_data = []\n",
    "count=0\n",
    "for item in dataset[\"train\"]:\n",
    "\n",
    "    if(count>=5):\n",
    "        break\n",
    "    horizontal_item = {\n",
    "        \"company_name\": item[\"company_name\"],\n",
    "        \"job_description\": item[\"job_description\"],\n",
    "        \"position_title\": item[\"position_title\"],\n",
    "        \"description_length\": item[\"description_length\"],\n",
    "        \"model_response\": item[\"model_response\"]\n",
    "    }\n",
    "    horizontal_data.append(horizontal_item)\n",
    "    count+=1\n",
    "\n",
    "# Saving the horizontal data to a JSON file\n",
    "with open(\"job_descriptions/cv_data.json\", \"w\") as file:\n",
    "    json.dump(horizontal_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data extraction from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "# Function to extract category, skills, and education from a PDF\n",
    "def extract_details(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Initialize variables to store extracted details\n",
    "            category = None\n",
    "            skills = []\n",
    "            education = []\n",
    "\n",
    "            # Iterate through pages in the PDF\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # Search for patterns in the extracted text\n",
    "                if \"Category\" in text:\n",
    "                    category = text.split(\"Category\")[1].strip()\n",
    "                if \"Skills\" in text:\n",
    "                    skills = [skill.strip() for skill in text.split(\"Skills\")[1].split(\",\")]\n",
    "                if \"Education\" in text:\n",
    "                    education = [edu.strip() for edu in text.split(\"Education\")[1].split(\";\")]\n",
    "\n",
    "            return {\n",
    "                'PDFFilename': os.path.basename(pdf_path), # Include the PDF filename\n",
    "                'Category': category,\n",
    "                'Skills': skills,\n",
    "                'Education': education,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details from {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing PDF CVs\n",
    "pdf_directory = 'archive/data/data/ENGINEERING'\n",
    "\n",
    "# Create a list to store extracted details\n",
    "all_details = []\n",
    "\n",
    "# Iterate through PDF files and extract details\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        details = extract_details(pdf_path)\n",
    "        if details:\n",
    "            print(f\"Details extracted from {filename}:\\n{details}\\n\")\n",
    "            all_details.append(details)\n",
    "\n",
    "# Save the extracted details in a JSON file\n",
    "output_file = 'extracted/extracted_details.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(all_details, json_file, indent=4)\n",
    "\n",
    "print(f\"Extracted details saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your CV data from cv_data.json\n",
    "data = {}\n",
    "with open(\"job_descriptions/cv_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "# Extract company names and job descriptions\n",
    "company_and_job_descriptions = {}\n",
    "for item in data:\n",
    "    company_and_job_descriptions[item['company_name']] = item['job_description']\n",
    "\n",
    "# Load DistilBERT tokenizer and model on the GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# Load job descriptions and extracted CV details from JSON\n",
    "with open('extracted/extracted_details.json', 'r') as json_file:\n",
    "    cv_details = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "# Create a list of job descriptions\n",
    "job_descriptions = list(company_and_job_descriptions.values())\n",
    "\n",
    "# Initialize a dictionary to store top 5 CVs for each job description\n",
    "top_5_cvs = {}\n",
    "\n",
    "# Tokenize and embed job descriptions\n",
    "job_desc_embeddings = [model(**tokenizer(job_desc, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1) for job_desc in job_descriptions]\n",
    "\n",
    "# Initialize a dictionary to store collected CVs for each job description\n",
    "collected_cvs = {job_desc: [] for job_desc in job_descriptions}\n",
    "\n",
    "# Tokenize and embed CV details\n",
    "for cv in cv_details:\n",
    "    # Iterate over job descriptions\n",
    "    for job_desc in job_descriptions:\n",
    "        cv_text = f\"{cv['Category']} {', '.join(cv['Skills'])} {', '.join(cv['Education'])}\"\n",
    "        cv_embedding = model(**tokenizer(cv_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Calculate cosine similarity between job descriptions and CVs using PyTorch\n",
    "        cv_embedding = cv_embedding.squeeze(0)  # Remove the batch dimension\n",
    "        job_desc_embedding = job_desc_embeddings[job_descriptions.index(job_desc)].squeeze(0)  # Get the corresponding job description embedding\n",
    "\n",
    "        similarity = torch.nn.functional.cosine_similarity(job_desc_embedding, cv_embedding, dim=0).item()\n",
    "\n",
    "        # Store the CV and similarity score\n",
    "        collected_cvs[job_desc].append((cv['PDFFilename'], similarity))\n",
    "\n",
    "# Sort the collected CVs by similarity score and select the top 5\n",
    "for job_desc, cvs in collected_cvs.items():\n",
    "    top_5_cvs[job_desc] = sorted(cvs, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Function to find the key (company name) for a given job description\n",
    "def find_the_key(job_description):\n",
    "    for key, value in company_and_job_descriptions.items():\n",
    "        if value == job_description:\n",
    "            return key\n",
    "\n",
    "# Print the top 5 CVs for each job description\n",
    "for job_desc, cvs in top_5_cvs.items():\n",
    "    company = find_the_key(job_desc)\n",
    "    print(f\"Top 5 CVs for '{company}':\")\n",
    "    for cv, similarity in cvs:\n",
    "        print(f\"CV: {cv}, Similarity Score: {similarity}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 CVs for each job description\n",
    "\n",
    "shortlisted_cvs={}\n",
    "for job_desc, cvs in top_5_cvs.items():\n",
    "    company = find_the_key(job_desc)\n",
    "    print(f\"Top 5 CVs for '{company}':\")\n",
    "    list_of_selected_resumes=[]\n",
    "    for cv, similarity in cvs:\n",
    "        print(f\"CV: {cv}, Similarity Score: {similarity}\")\n",
    "        list_of_selected_resumes.append(cv)\n",
    "    shortlisted_cvs[company]=list_of_selected_resumes\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortlisted_cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shortlisted/shortlisted_cvs.json\", \"w\") as file:\n",
    "    json.dump(shortlisted_cvs, file, indent=4)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
