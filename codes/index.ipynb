{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules and libraries in the code. \n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os \n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the huggin face model. \n",
    "dataset = load_dataset(\"jacob-hugging-face/job-descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the output and the data format of the dataset. The data was in the form of Nested Dictionary.\n",
    "print(dataset['train'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionry data into json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to properly transform the data to later save it into json format. \n",
    "def transform_to_horizontal(data, fields, limit=None):\n",
    "\n",
    "    # Initialzie list to hold the dataq. \n",
    "    horizontal_data = []\n",
    "    \n",
    "    # Use teh enumerate method to get the incdex as well as the item. \n",
    "    for idx, item in enumerate(data):\n",
    "\n",
    "        # Break the loop if it crosses the limit. \n",
    "        if limit is not None and idx >= limit:\n",
    "            break\n",
    "        # Append the items into the dictionary. \n",
    "        horizontal_item = {field: item[field] for field in fields}\n",
    "        # Append the dictionary data as elements of the list. \n",
    "        horizontal_data.append(horizontal_item)\n",
    "\n",
    "    # Return the list object. \n",
    "    return horizontal_data\n",
    "\n",
    "# Specify the fields you want to include in the horizontal format\n",
    "fields_to_include = [\"company_name\", \"job_description\", \"position_title\", \"description_length\", \"model_response\"]\n",
    "\n",
    "# Transform the data to horizontal format\n",
    "horizontal_data = transform_to_horizontal(dataset[\"train\"], fields_to_include, limit=15)\n",
    "\n",
    "# Saving the horizontal data to a JSON file\n",
    "with open(\"job_descriptions/cv_data.json\", \"w\") as file:\n",
    "    # Dump the json data into a file. \n",
    "    json.dump(horizontal_data, file)\n",
    "    # Its always a good practice to close the file after the operations are done. \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data extraction from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path from which you need to extract all the PDFs. \n",
    "PATH=\"archive/data/data\"\n",
    "\n",
    "# Extract all the directories under the mentioned path using list comprehension method. \n",
    "items = os.listdir(PATH)\n",
    "directories = [item for item in items if os.path.isdir(os.path.join(PATH, item))]\n",
    "\n",
    "print(\"Directories in the path:\")\n",
    "\n",
    "# Initialize a list which will hold the list of all the categories (basically the names of the directories)\n",
    "list_of_categories=[]\n",
    "\n",
    "for directory in directories:\n",
    "    list_of_categories.append(directory)\n",
    "    # print(directory)\n",
    "print(list_of_categories)\n",
    "\n",
    "\n",
    "# Function to extract category, skills, and education from a PDF\n",
    "def extract_details(pdf_path):\n",
    "    # Wrap the code in try catch block to avoid any error. \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Initialize variables to store extracted details\n",
    "            category = None\n",
    "            skills = []\n",
    "            education = []\n",
    "\n",
    "            # Iterate through pages in the PDF\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # Search for patterns in the extracted text\n",
    "                if \"Category\" in text:\n",
    "                    category = text.split(\"Category\")[1].strip()\n",
    "                if \"Skills\" in text:\n",
    "                    skills = [skill.strip() for skill in text.split(\"Skills\")[1].split(\",\")]\n",
    "                if \"Education\" in text:\n",
    "                    education = [edu.strip() for edu in text.split(\"Education\")[1].split(\";\")]\n",
    "            # Return the data in the form of dictionary. \n",
    "            return {\n",
    "                'PDFFilename': os.path.basename(pdf_path), # Include the PDF filename\n",
    "                'Category': category,\n",
    "                'Skills': skills,\n",
    "                'Education': education,\n",
    "            }\n",
    "    # Create an exception and return None in that case. \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details from {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create a list to store extracted details\n",
    "all_details = []\n",
    "\n",
    "for directory in list_of_categories:\n",
    "    # Directory containing PDF CVs\n",
    "    pdf_directory = f'archive/data/data/{directory}'\n",
    "    # Iterate through PDF files and extract details\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            details = extract_details(pdf_path)\n",
    "            if details:\n",
    "                print(f\"Details extracted from {filename}:\\n{details}\\n\")\n",
    "                all_details.append(details)\n",
    "\n",
    "# Save the extracted details in a JSON file\n",
    "output_file = 'extracted/extracted_details.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    # Write the json data into a file. \n",
    "    json.dump(all_details, json_file, indent=4)\n",
    "\n",
    "    # Close the file.\n",
    "    json_file.close()\n",
    "# Print the output file \n",
    "print(f\"Extracted details saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory for Memory optimization. \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load your CV data from cv_data.json\n",
    "data = {}\n",
    "with open(\"job_descriptions/cv_data.json\", \"r\") as file:\n",
    "    # Load json data from the file. \n",
    "    data = json.load(file)\n",
    "    # Close the file. \n",
    "    file.close()\n",
    "\n",
    "# Extract company names and job descriptions. This will hold the companry name along with the job description.\n",
    "company_and_job_descriptions = {}\n",
    "for item in data:\n",
    "    company_and_job_descriptions[item['company_name']] = item['job_description']\n",
    "\n",
    "# Load DistilBERT tokenizer and model on the GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Use the distilbert-base-uncased model \n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# Load job descriptions and extracted CV details from JSON\n",
    "with open('extracted/extracted_details.json', 'r') as json_file:\n",
    "    # Load the json file. \n",
    "    cv_details = json.load(json_file)\n",
    "    # Close the file. \n",
    "    json_file.close()\n",
    "\n",
    "# Create a list of job descriptions\n",
    "job_descriptions = list(company_and_job_descriptions.values())\n",
    "\n",
    "# Initialize a dictionary to store top 5 CVs for each job description\n",
    "top_5_cvs = {}\n",
    "\n",
    "# Tokenize and embed job descriptions using the tokenizer, model and the list comprehension technique. \n",
    "job_desc_embeddings = [model(**tokenizer(job_desc, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1) for job_desc in job_descriptions]\n",
    "\n",
    "# Initialize a dictionary to store collected CVs for each job description\n",
    "collected_cvs = {job_desc: [] for job_desc in job_descriptions}\n",
    "\n",
    "# Tokenize and embed CV details\n",
    "for cv in cv_details:\n",
    "    # Iterate over job descriptions\n",
    "    for job_desc in job_descriptions:\n",
    "\n",
    "        '''Use the join method to add categoiry, skills, and education as text data. This will be the raw text to be tokenized representing most of the information of the user. '''\n",
    "\n",
    "        cv_text = f\"{cv['Category']} {', '.join(cv['Skills'])} {', '.join(cv['Education'])}\"\n",
    "\n",
    "        # Create embeddings of the raw text processed in the earlier step. The model is made to run on GPU if available. \n",
    "        # Padding is set to true to match the text datas. truncation is set to True to ensuer that the texts lies within a max limit. Max length defines the maximum length of the texts. \n",
    "        cv_embedding = model(**tokenizer(cv_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Calculate cosine similarity between job descriptions and CVs using PyTorch\n",
    "        cv_embedding = cv_embedding.squeeze(0)  # Remove the batch dimension\n",
    "\n",
    "        job_desc_embedding = job_desc_embeddings[job_descriptions.index(job_desc)].squeeze(0)  # Get the corresponding job description embedding\n",
    "        \n",
    "        # Perform similarity operation using the cosine similary of PyTorch. Note that I used this instead of the sklearn implementation to run the operation of GPU devices. This will help to perform the operation much faster. \n",
    "        similarity = torch.nn.functional.cosine_similarity(job_desc_embedding, cv_embedding, dim=0).item()\n",
    "\n",
    "        # Store the CV and similarity score\n",
    "        collected_cvs[job_desc].append((cv['PDFFilename'], similarity))\n",
    "\n",
    "# Sort the collected CVs by similarity score and select the top 5\n",
    "for job_desc, cvs in collected_cvs.items():\n",
    "    top_5_cvs[job_desc] = sorted(cvs, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Function to find the key (company name) for a given job description\n",
    "def find_the_key(job_description):\n",
    "    for key, value in company_and_job_descriptions.items():\n",
    "        if value == job_description:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 CVs for each job description\n",
    "\n",
    "shortlisted_cvs={}\n",
    "for job_desc, cvs in top_5_cvs.items():\n",
    "\n",
    "    # Call the function to find the company name corresponding to the job description. \n",
    "    company = find_the_key(job_desc)\n",
    "    print(f\"Top 5 CVs for '{company}':\")\n",
    "\n",
    "    # List to store all the selected resumes. \n",
    "    list_of_selected_resumes=[]\n",
    "    for cv, similarity in cvs:\n",
    "        print(f\"CV: {cv}, Similarity Score: {similarity}\")\n",
    "        list_of_selected_resumes.append(cv)\n",
    "    shortlisted_cvs[company]=list_of_selected_resumes\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortlisted_cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cvs of the final shortlisted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shortlisted/shortlisted_cvs.json\", \"w\") as file:\n",
    "    json.dump(shortlisted_cvs, file, indent=4)\n",
    "    # Close the file. \n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
