{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jacob-hugging-face/job-descriptions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionry data into json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data into horizontal format\n",
    "import json\n",
    "horizontal_data = []\n",
    "count=0\n",
    "for item in dataset[\"train\"]:\n",
    "\n",
    "    if(count>=5):\n",
    "        break\n",
    "    horizontal_item = {\n",
    "        \"company_name\": item[\"company_name\"],\n",
    "        \"job_description\": item[\"job_description\"],\n",
    "        \"position_title\": item[\"position_title\"],\n",
    "        \"description_length\": item[\"description_length\"],\n",
    "        \"model_response\": item[\"model_response\"]\n",
    "    }\n",
    "    horizontal_data.append(horizontal_item)\n",
    "    count+=1\n",
    "\n",
    "# Saving the horizontal data to a JSON file\n",
    "with open(\"cv_data.json\", \"w\") as file:\n",
    "    json.dump(horizontal_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data extraction from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "# Function to extract category, skills, and education from a PDF\n",
    "def extract_details(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Initialize variables to store extracted details\n",
    "            category = None\n",
    "            skills = []\n",
    "            education = []\n",
    "\n",
    "            # Iterate through pages in the PDF\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # Search for patterns in the extracted text\n",
    "                if \"Category\" in text:\n",
    "                    category = text.split(\"Category\")[1].strip()\n",
    "                if \"Skills\" in text:\n",
    "                    skills = [skill.strip() for skill in text.split(\"Skills\")[1].split(\",\")]\n",
    "                if \"Education\" in text:\n",
    "                    education = [edu.strip() for edu in text.split(\"Education\")[1].split(\";\")]\n",
    "\n",
    "            return {\n",
    "                'PDFFilename': os.path.basename(pdf_path), # Include the PDF filename\n",
    "                'Category': category,\n",
    "                'Skills': skills,\n",
    "                'Education': education,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details from {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Directory containing PDF CVs\n",
    "pdf_directory = '/media/asifr/work/cv matches/archive/data/data/ENGINEERING'\n",
    "\n",
    "# Create a list to store extracted details\n",
    "all_details = []\n",
    "\n",
    "# Iterate through PDF files and extract details\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        details = extract_details(pdf_path)\n",
    "        if details:\n",
    "            print(f\"Details extracted from {filename}:\\n{details}\\n\")\n",
    "            all_details.append(details)\n",
    "\n",
    "# Save the extracted details in a JSON file\n",
    "output_file = 'extracted/extracted_details.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(all_details, json_file, indent=4)\n",
    "\n",
    "print(f\"Extracted details saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/media/asifr/work/cv matches/codes/index.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/asifr/work/cv%20matches/codes/index.ipynb#X26sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(cv_embedding))\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/asifr/work/cv%20matches/codes/index.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(job_desc_embeddings))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/asifr/work/cv%20matches/codes/index.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m similarities \u001b[39m=\u001b[39m cosine_similarity(job_desc_embeddings, cv_embedding)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/asifr/work/cv%20matches/codes/index.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Find the top 5 CVs for each job description\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/asifr/work/cv%20matches/codes/index.ipynb#X26sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, job_desc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(job_descriptions):\n",
      "File \u001b[0;32m/media/asifr/work/cv matches/venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/media/asifr/work/cv matches/venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1577\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m \n\u001b[1;32m   1544\u001b[0m \u001b[39mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[39m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1577\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m   1579\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/media/asifr/work/cv matches/venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:149\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_pairwise_arrays\u001b[39m(\n\u001b[1;32m     74\u001b[0m     X,\n\u001b[1;32m     75\u001b[0m     Y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m ):\n\u001b[1;32m     83\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set X and Y appropriately and checks inputs.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[39m    If Y is None, it is set as a pointer to X (i.e. not a copy).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m        If Y was None, safe_Y will be a pointer to X.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     X, Y, dtype_float \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m    151\u001b[0m     estimator \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheck_pairwise_arrays\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/asifr/work/cv matches/venv/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:55\u001b[0m, in \u001b[0;36m_return_float_dtype\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m1. If dtype of X and Y is float32, then dtype float32 is returned.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m2. Else dtype float is returned.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m---> 55\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(X)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     Y_dtype \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m/media/asifr/work/cv matches/venv/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Load job descriptions and extracted CV details from JSON\n",
    "with open('extracted_details.json', 'r') as json_file:\n",
    "    cv_details = json.load(json_file)\n",
    "\n",
    "# Sample job descriptions (replace with your own data)\n",
    "job_descriptions = [\n",
    "    \"We are looking for a software engineer with expertise in Python and machine learning.\",\n",
    "    \"Seeking a project manager with a background in construction management.\",\n",
    "    \"Hiring a data scientist with experience in deep learning and natural language processing.\",\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store top 5 CVs for each job description\n",
    "top_5_cvs = {}\n",
    "\n",
    "# Tokenize and embed job descriptions\n",
    "job_desc_embeddings = [model(**tokenizer(job_desc, return_tensors='pt')).last_hidden_state.mean(dim=1) for job_desc in job_descriptions]\n",
    "\n",
    "# Tokenize and embed CV details\n",
    "for cv in cv_details:\n",
    "    cv_text = f\"{cv['Category']} {', '.join(cv['Skills'])} {', '.join(cv['Education'])}\"\n",
    "    cv_embedding = model(**tokenizer(cv_text, return_tensors='pt')).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between job descriptions and CVs\n",
    "    cv_embedding=cv_embedding.detach().numpy()\n",
    "\n",
    "    print(type(cv_embedding))\n",
    "    print(type(job_desc_embeddings))\n",
    "    similarities = cosine_similarity(job_desc_embeddings, cv_embedding)\n",
    "\n",
    "    # Find the top 5 CVs for each job description\n",
    "    for i, job_desc in enumerate(job_descriptions):\n",
    "        if job_desc not in top_5_cvs:\n",
    "            top_5_cvs[job_desc] = []\n",
    "\n",
    "        top_5_indices = similarities[i].argsort()[-5:][::-1]\n",
    "        top_5_cvs[job_desc].extend([(cv['PDFFilename'], similarities[i][idx]) for idx in top_5_indices])\n",
    "\n",
    "# Print the top 5 CVs for each job description\n",
    "for job_desc, cvs in top_5_cvs.items():\n",
    "    print(f\"Top 5 CVs for '{job_desc}':\")\n",
    "    for cv in cvs:\n",
    "        print(f\"CV: {cv[0]}, Similarity Score: {cv[1]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
